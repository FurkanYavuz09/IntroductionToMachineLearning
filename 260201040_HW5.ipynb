{"cells":[{"cell_type":"markdown","source":["# HW-5: Malware Classification (Due 5th January, 2023)\n","\n","**Instructions:**\n","\n","Suppose your company is struggling with a series of computer virus attacks for the past several months. The viruses were grouped into a few types with some effort. However, it takes a long time to sort out what kind of virus it is when been hit with. Thus, as a senior IT department member, you undertook a project to classify the virus as quickly as possible. You've been given a dataset of the features that may be handy (or not), and  also the associated virus type (target variable). \n","\n","You are supposed to try different classification methods and apply best practices we have seen in the lectures such as grid search, cross validation, regularization etc. To increase your grade you can add more elaboration such as using ensembling or exploiting feature selection/extraction techniques. **An evaluation rubric is provided.**\n","\n","Please prepare a python notebook that describes the steps, present the results as well as your comments. \n","\n","You can download the data (csv file) [here](https://drive.google.com/file/d/1yxbibzUU8bjOyChDVFPfQ4viLduYdk29/view?usp=sharing).\n"],"metadata":{"id":"YDkn--vjlx9I"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from sklearn import tree\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn import svm\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.ensemble import VotingClassifier\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.feature_selection import SelectKBest\n","from sklearn.feature_selection import f_classif\n","from sklearn.metrics import accuracy_score\n","from sklearn.feature_selection import mutual_info_regression\n","from sklearn.model_selection import cross_val_score\n","\n"],"metadata":{"id":"8BmW68k4fZuN","executionInfo":{"status":"ok","timestamp":1673206273284,"user_tz":-180,"elapsed":495,"user":{"displayName":"Furkan Yavuz","userId":"12888749653335336604"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["df = pd.read_csv('hw5.csv')\n","\n","X = df.drop(\"target\", axis=1)\n","y = df['target']\n","\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)#Split the data as train and test\n"],"metadata":{"id":"KyOXFogXffO6","executionInfo":{"status":"ok","timestamp":1673205983366,"user_tz":-180,"elapsed":1337,"user":{"displayName":"Furkan Yavuz","userId":"12888749653335336604"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["from sklearn import svm\n","\n","\n","#Hyperparameters set\n","\n","dt_parameters = {\n","    'criterion':['gini','entropy'],\n","    'max_depth': [2, 4, 6],\n","    'min_samples_split': [2, 4, 6],\n","    'min_samples_leaf': range(1,5)\n","}\n","\n","svm_parameters =  {'C': [0.1,1, 10, 100], 'gamma': [1,0.1,0.01,0.001],}#When I include Kernel parameter run time extremely increase\n","\n","rf_parameters = { \n","    \n","    'max_features': ['auto'],\n","    'max_depth' : [4,5,6,7,],\n","    'criterion' :['gini', 'entropy']\n","}\n","\n","\n","knn_parameters = {\n","    'n_neighbors' : [3,5,11,19],\n","    'weights' : ['uniform', 'distance'],\n","    'metric' : ['euclidean', 'manhattan']\n","}\n","\n","\n","\n","\n","\n","dt = tree.DecisionTreeClassifier()\n","knn = KNeighborsClassifier()\n","svm = svm.SVC()\n","rf = RandomForestClassifier()\n","nb = GaussianNB()\n","lr = LogisticRegression(max_iter=100000)\n","\n","\n","\n","dt_model = GridSearchCV(dt, dt_parameters, n_jobs=-1)\n","knn_model = GridSearchCV(knn, knn_parameters, n_jobs=-1)\n","svm_model = GridSearchCV(svm, svm_parameters, n_jobs=-1)\n","rf_model = GridSearchCV(rf, rf_parameters, n_jobs=-1)\n","\n","\n","\n","\n","\n","ensemble = VotingClassifier(estimators=[('dt', dt_model), ('knn', knn_model), ('rf', rf_model), ('nb', nb)], voting='hard')#Ensembling applied with the algorithms\n","\n","selected_features= []#Features selected with different reduced sizes.\n","\n","for n in [25, 50, 100, 200]:\n","    \n","    selector = SelectKBest(f_classif, k=n)\n","\n","    selector.fit(X_train, y_train)\n","\n","    X_train_selected = selector.transform(X_train)\n","    \n","    X_test_selected = selector.transform(X_test)\n","\n","    selected_features.append([X_train_selected, X_test_selected])\n","\n","\n","lr.fit(selected_features[0][0], y_train)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_698lZj2XVCk","executionInfo":{"status":"ok","timestamp":1673210226851,"user_tz":-180,"elapsed":70483,"user":{"displayName":"Furkan Yavuz","userId":"12888749653335336604"}},"outputId":"db7dd596-8b7f-45ac-b6e8-356f23f75fd4"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of f AND g EVALUATIONS EXCEEDS LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n"]},{"output_type":"execute_result","data":{"text/plain":["LogisticRegression(max_iter=100000)"]},"metadata":{},"execution_count":40}]},{"cell_type":"code","source":["#K-fold cross validation applied\n","scores_knn = cross_val_score(knn, X_train, y_train, cv=5, scoring='accuracy')\n","print(\"Knn cross validation score :\",scores_knn.max())\n","\n","scores_rf = cross_val_score(rf, X_train, y_train, cv=5, scoring='accuracy')\n","print(\"Random Forest cross validation score :\", scores_rf.max())\n","\n","scores_svm = cross_val_score(svm, X_train, y_train, cv=3, scoring='accuracy')#I choose k = 3 because of runtime\n","print(\"Svm cross validation score :\", scores_svm.max())\n","\n","scores_dt = cross_val_score(dt, X_train, y_train, cv=5, scoring='accuracy')\n","print(\"Decision Tree cross validation score :\", scores_dt.max())\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1D-TP7Xf8EPC","executionInfo":{"status":"ok","timestamp":1673208940993,"user_tz":-180,"elapsed":103038,"user":{"displayName":"Furkan Yavuz","userId":"12888749653335336604"}},"outputId":"cebd29ac-2832-4d6e-b411-65ce69f59b2b"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["Knn cross validation score : 0.765\n","Random Forest cross validation score : 0.9148936170212766\n","Svm cross validation score : 0.36534133533383345\n","Decision Tree cross validation score : 0.80875\n"]}]},{"cell_type":"code","source":["size = [25, 50, 100, 200]\n","n = 0\n","for selected in selected_features:\n","\n","\n","    dt_model.fit(selected[0], y_train)\n","    dt_predictions = dt_model.predict(selected[1])\n","    print(\"Decision Tree n=\",size[n],\"Score :\", accuracy_score(y_test, dt_predictions))\n","    # print best parameter after tuning\n","    print(dt_model.best_params_)\n","\n","\n","    knn_model.fit(selected[0], y_train)\n","    knn_predictions = knn_model.predict(selected[1])\n","    print(\"KNN n=\",size[n], \"Score :\", accuracy_score(y_test, knn_predictions))\n","    # print best parameter after tuning\n","    print(knn_model.best_params_)  \n","\n","\n","    svm_model.fit(selected[0], y_train)\n","    svm_predictions = svm_model.predict(selected[1])\n","    print(\"SVM n=\",size[n], \"Score :\", accuracy_score(y_test, svm_predictions))\n","    # print best parameter after tuning\n","    print(svm_model.best_params_)\n","\n","\n","    rf_model.fit(selected[0], y_train)\n","    rf_predictions = rf_model.predict(selected[1])\n","    print(\"Random Forest n=\",size[n], \"Score :\", accuracy_score(y_test, rf_predictions))\n","    # print best parameter after tuning\n","    print(rf_model.best_params_)\n","\n","    nb.fit(selected[0], y_train)\n","    nb_predictions = nb.predict(selected[1])\n","    print(\"Naive Bayes n=\",size[n], \"Score :\", accuracy_score(y_test, nb_predictions))\n","    \n","\n","    \n","    lr_predictions = lr.predict(selected[1])\n","    print(\"Logistic Regression n=\",size[n], \"Score :\", accuracy_score(y_test, lr_predictions))\n","\n","    ensemble.fit(selected[0], y_train)\n","    ensemble_predictions = ensemble.predict(selected[1])\n","    print(\"Ensemble n=\",size[n], \"Score :\", accuracy_score(y_test, ensemble_predictions))\n","\n","    print(\"\\n\")\n","    n+=1\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"Nh31hX8_8Bn3","executionInfo":{"status":"error","timestamp":1673211562620,"user_tz":-180,"elapsed":685908,"user":{"displayName":"Furkan Yavuz","userId":"12888749653335336604"}},"outputId":"c8a95528-3d8f-4fc0-c1e3-efcb2a86f01f"},"execution_count":45,"outputs":[{"output_type":"stream","name":"stdout","text":["Decision Tree n= 25 Score : 0.836\n","{'criterion': 'entropy', 'max_depth': 6, 'min_samples_leaf': 1, 'min_samples_split': 4}\n","KNN n= 25 Score : 0.858\n","{'metric': 'manhattan', 'n_neighbors': 11, 'weights': 'distance'}\n","SVM n= 25 Score : 0.278\n","{'C': 0.1, 'gamma': 1}\n","Random Forest n= 25 Score : 0.89\n","{'criterion': 'entropy', 'max_depth': 7, 'max_features': 'auto'}\n","Naive Bayes n= 25 Score : 0.66\n","Logistic Regression n= 25 Score : 0.716\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of f AND g EVALUATIONS EXCEEDS LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n"]},{"output_type":"stream","name":"stdout","text":["Ensemble n= 25 Score : 0.876\n","\n","\n","Decision Tree n= 50 Score : 0.847\n","{'criterion': 'entropy', 'max_depth': 6, 'min_samples_leaf': 4, 'min_samples_split': 2}\n","KNN n= 50 Score : 0.869\n","{'metric': 'manhattan', 'n_neighbors': 5, 'weights': 'distance'}\n","SVM n= 50 Score : 0.278\n","{'C': 0.1, 'gamma': 1}\n","Random Forest n= 50 Score : 0.9\n","{'criterion': 'entropy', 'max_depth': 7, 'max_features': 'auto'}\n","Naive Bayes n= 50 Score : 0.62\n"]},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-45-9c24366392c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mlr_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselected\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Logistic Regression n=\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Score :\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_predictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    423\u001b[0m             \u001b[0mVector\u001b[0m \u001b[0mcontaining\u001b[0m \u001b[0mthe\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m         \"\"\"\n\u001b[0;32m--> 425\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdense_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintercept_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcheck_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ensure_2d\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_n_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_check_n_features\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_features_in_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    401\u001b[0m                 \u001b[0;34mf\"X has {n_features} features, but {self.__class__.__name__} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m                 \u001b[0;34mf\"is expecting {self.n_features_in_} features as input.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: X has 50 features, but LogisticRegression is expecting 25 features as input."]}]},{"cell_type":"markdown","source":["# Hyperparameter search and scores\n","\n","Decision Tree n= 25 Score : 0.836\n","{'criterion': 'entropy', 'max_depth': 6, 'min_samples_leaf': 1, 'min_samples_split': 4}\n","\n","KNN n= 25 Score : 0.858\n","{'metric': 'manhattan', 'n_neighbors': 11, 'weights': 'distance'}\n","\n","SVM n= 25 Score : 0.278\n","{'C': 0.1, 'gamma': 1}\n","\n","Random Forest n= 25 Score : 0.89\n","{'criterion': 'entropy', 'max_depth': 7, 'max_features': 'auto'}\n","\n","Naive Bayes n= 25 Score : 0.66\n","\n","Logistic Regression n= 25 Score : 0.716\n","\n","Ensemble n= 25 Score : 0.876\n","\n","\n","\n","I get the highest score with using Random Forest ."],"metadata":{"id":"z9Cy_xTZCHwa"}}],"metadata":{"colab":{"provenance":[{"file_id":"1ekkdt4NJRpu3Farh-cUQZmoP3q1eRKWG","timestamp":1536031133585},{"file_id":"1DziSbVqaWErcHDGNfhjupryp5jZFd-t2","timestamp":1535684058223}]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"nteract":{"version":"nteract-front-end@1.0.0"},"accelerator":"GPU","gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}